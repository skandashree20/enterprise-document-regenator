{
  "name": "04 - Batch Router",
  "nodes": [
    {
      "parameters": {},
      "id": "trigger-node",
      "name": "Execute Workflow Trigger",
      "type": "n8n-nodes-base.executeWorkflowTrigger",
      "typeVersion": 1,
      "position": [
        0,
        0
      ]
    },
    {
      "parameters": {
        "jsCode": "// Process documents and extract text content\nconst items = $input.all();\n\nconst processedDocs = await Promise.all(items.map(async (item) => {\n  const data = item.json;\n  const binary = item.binary;\n  \n  // If there's an error from fetch, pass it through\n  if (data.status === 'error') {\n    return { json: data };\n  }\n  \n  const extension = (data.extension || data.file_name?.split('.').pop() || '').toLowerCase();\n  let extractedText = '';\n  let processorType = 'generic';\n  let debugInfo = { \n    hasBinary: !!binary, \n    binaryKeys: binary ? Object.keys(binary) : [],\n    hasBinaryDataInJson: !!data.binary_data\n  };\n  \n  try {\n    // Get base64 data - try from JSON first (more reliable through Execute Workflow),\n    // then fall back to binary property\n    let base64Data = null;\n    \n    // Method 1: Check if binary_data is in JSON (passed through Execute Workflow)\n    if (data.binary_data) {\n      base64Data = data.binary_data;\n      debugInfo.source = 'json_binary_data';\n    }\n    // Method 2: Check n8n binary property\n    else if (binary) {\n      const binaryKey = Object.keys(binary)[0];\n      if (binaryKey && binary[binaryKey]?.data) {\n        base64Data = binary[binaryKey].data;\n        debugInfo.source = 'binary_property';\n        debugInfo.binaryKey = binaryKey;\n      }\n    }\n    \n    if (base64Data) {\n      // Convert base64 to buffer\n      const buffer = Buffer.from(base64Data, 'base64');\n      debugInfo.bufferLength = buffer.length;\n      \n      if (['txt', 'md', 'csv', 'json'].includes(extension)) {\n        // Plain text files - direct conversion\n        extractedText = buffer.toString('utf-8');\n        processorType = 'text';\n      } else if (['docx', 'doc'].includes(extension)) {\n        // DOCX files are ZIP archives containing XML\n        // The text is in word/document.xml which is deflate-compressed\n        // We'll scan for any readable text patterns\n        const rawContent = buffer.toString('binary');\n        const textChunks = [];\n        \n        // Method 1: Look for uncompressed XML text tags\n        const xmlMatches = rawContent.match(/<w:t[^>]*>([^<]+)<\\/w:t>/g) || [];\n        for (const m of xmlMatches) {\n          const text = m.replace(/<[^>]+>/g, '').trim();\n          if (text.length > 0) textChunks.push(text);\n        }\n        debugInfo.xmlMatchCount = xmlMatches.length;\n        \n        // Method 2: Look for strings in shared strings (xlsx-style, some docx use this)\n        const siMatches = rawContent.match(/<t>([^<]+)<\\/t>/g) || [];\n        for (const m of siMatches) {\n          const text = m.replace(/<[^>]+>/g, '').trim();\n          if (text.length > 0 && !textChunks.includes(text)) textChunks.push(text);\n        }\n        debugInfo.siMatchCount = siMatches.length;\n        \n        // Method 3: Scan for readable ASCII sequences (words 4+ chars)\n        // This catches text that might be in different XML patterns\n        const wordRegex = /[A-Za-z]{4,}(?:[A-Za-z0-9]*)/g;\n        const wordMatches = rawContent.match(wordRegex) || [];\n        const uniqueWords = [...new Set(wordMatches)].filter(w => \n          w.length >= 4 && \n          !/^[A-Z]{2,}[a-z]+$/.test(w) && // Filter out CamelCase XML tags\n          !w.match(/^(xmlns|http|rels|schema|docx|word|document|content|types|xml)/i)\n        );\n        debugInfo.uniqueWordCount = uniqueWords.length;\n        \n        // Combine results\n        if (textChunks.length > 0) {\n          extractedText = textChunks.join(' ');\n        } else if (uniqueWords.length > 20) {\n          // Fallback: use extracted words if we found enough\n          extractedText = uniqueWords.join(' ');\n        }\n        \n        extractedText = extractedText.replace(/\\s+/g, ' ').trim();\n        processorType = 'docx';\n      } else if (extension === 'pdf') {\n        // PDF text extraction\n        const content = buffer.toString('latin1');\n        const textParts = [];\n        \n        // Look for text in PDF streams\n        const streamRegex = /stream[\\r\\n]+([\\s\\S]*?)[\\r\\n]+endstream/g;\n        let match;\n        while ((match = streamRegex.exec(content)) !== null) {\n          const cleaned = match[1].replace(/[^\\x20-\\x7E\\n]/g, ' ').replace(/\\s+/g, ' ').trim();\n          if (cleaned.length > 20) textParts.push(cleaned);\n        }\n        \n        // Also look for text objects BT...ET\n        const textObjRegex = /BT[\\s\\S]*?\\(([^)]+)\\)[\\s\\S]*?ET/g;\n        while ((match = textObjRegex.exec(content)) !== null) {\n          if (match[1] && match[1].length > 2) textParts.push(match[1]);\n        }\n        \n        extractedText = textParts.join(' ').replace(/\\s+/g, ' ').substring(0, 20000);\n        processorType = 'pdf';\n      } else if (['pptx', 'ppt'].includes(extension)) {\n        const rawContent = buffer.toString('binary');\n        const textMatches = rawContent.match(/<a:t>([^<]+)<\\/a:t>/g) || [];\n        extractedText = textMatches.map(m => m.replace(/<[^>]+>/g, '')).join(' ');\n        processorType = 'pptx';\n      } else if (['xlsx', 'xls'].includes(extension)) {\n        const rawContent = buffer.toString('binary');\n        const textMatches = rawContent.match(/<t[^>]*>([^<]+)<\\/t>/g) || [];\n        extractedText = textMatches.map(m => m.replace(/<[^>]+>/g, '')).join(' ');\n        processorType = 'xlsx';\n      }\n    } else {\n      debugInfo.noBinaryData = true;\n    }\n  } catch (err) {\n    extractedText = `Error extracting text: ${err.message}`;\n    debugInfo.error = err.message;\n  }\n  \n  // If still no text, create informative placeholder\n  if (!extractedText || extractedText.length < 50) {\n    extractedText = `Document: ${data.file_name}\\n` +\n      `Type: ${extension.toUpperCase()} file\\n` +\n      `Size: ${data.file_size ? Math.round(data.file_size/1024) + ' KB' : 'unknown'}\\n` +\n      `MIME: ${data.mime_type || 'unknown'}\\n` +\n      `Note: Binary content could not be fully extracted. This is a ${extension} document that requires specialized parsing.\\n` +\n      `Debug: ${JSON.stringify(debugInfo)}`;\n  }\n  \n  return {\n    json: {\n      file_id: data.file_id,\n      file_name: data.file_name,\n      mime_type: data.mime_type,\n      extension: extension,\n      file_size: data.file_size,\n      status: 'processed',\n      processor_type: processorType,\n      extracted_text: extractedText.substring(0, 50000),\n      text_length: extractedText.length,\n      processed_at: new Date().toISOString(),\n      debug: debugInfo\n    }\n  };\n}));\n\nreturn [{\n  json: {\n    processed_documents: processedDocs.map(d => d.json),\n    batch_complete: true,\n    processed_count: processedDocs.filter(d => d.json.status !== 'error').length,\n    error_count: processedDocs.filter(d => d.json.status === 'error').length\n  }\n}];"
      },
      "id": "process-docs",
      "name": "Process Documents",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        220,
        0
      ]
    }
  ],
  "connections": {
    "Execute Workflow Trigger": {
      "main": [
        [
          {
            "node": "Process Documents",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "settings": {
    "executionOrder": "v1"
  },
  "staticData": null,
  "tags": [
    {
      "name": "processing"
    }
  ],
  "triggerCount": 0,
  "updatedAt": "2024-01-01T00:00:00.000Z",
  "versionId": "1",
  "active": false
}